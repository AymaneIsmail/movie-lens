# --------------------------------------------------------------------------------------
# Modifications apportÃ©es pour Ã©viter l'erreur "java.lang.OutOfMemoryError: Java heap space":
# 1. Configuration des ressources Spark :
#    - RÃ©duction de la mÃ©moire du driver Ã  3 Go (`spark.driver.memory`).
#    - Allocation de 10 exÃ©cutants avec 2 cÅ“urs chacun et 1 Go de mÃ©moire par exÃ©cutant 
#      (`spark.executor.instances`, `spark.executor.cores`, `spark.executor.memory`).
# 2. Activation de la sÃ©rialisation Kryo pour une gestion plus efficace de la mÃ©moire 
#    (`spark.serializer`).
# 3. DÃ©finition d'un rÃ©pertoire de checkpoint pour limiter la longueur de la lignÃ©e des RDDs 
#    et Ã©viter les fuites de mÃ©moire (`spark.sparkContext.setCheckpointDir("/tmp")`).
# 4. Ajustement du nombre de partitions des donnÃ©es pour une meilleure parallÃ©lisation 
#    (`ratings.repartition(200)`).
# --------------------------------------------------------------------------------------

from pyspark.sql import SparkSession
from pyspark.ml.recommendation import ALS
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
hdfs_prefix = "hdfs:///"  

# Chemins relatifs
ratings_path = f"{hdfs_prefix}/processed/rating.csv"
movies_path  = f"{hdfs_prefix}/processed/movie.csv"

# Train/test split
train_ratio = 0.8
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

def main():
    print(f"â–º ratings = {ratings_path}")
    print(f"â–º movies  = {movies_path or 'â€” pas de metadata â€”'}")
    print(f"â–º Train/Test split = {train_ratio:.2f}/{1-train_ratio:.2f}\n")

    # 1. DÃ©marrage Spark
    spark = SparkSession.builder \
        .appName("ALS Hyperparameter Tuning") \
        .config("spark.executor.instances", "10") \
        .config("spark.executor.cores", "2") \
        .config("spark.executor.memory", "1g") \
        .config("spark.driver.memory", "3g") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .getOrCreate()

    spark.sparkContext.setCheckpointDir("/tmp")

    # 2. Lecture des ratings
    ratings = (
        spark.read
             .option("header", True)
             .option("inferSchema", True)
             .csv(ratings_path)
             .selectExpr(
                 "cast(userId  as int)    as userId",
                 "cast(movieId as int)    as movieId",
                 "cast(rating  as float)  as rating"
             )
    ).repartition(200)

    # 3. Lecture des films si dÃ©finis
    if movies_path:
        movies = spark.read.option("header", True).csv(movies_path)
        print("AperÃ§u des films :")
        movies.show(5, truncate=False)

    # 4. Split train/test
    train, test = ratings.randomSplit([train_ratio, 1-train_ratio], seed=42)
    print(f"Count â†’ train: {train.count()}, test: {test.count()}")

    # 5. ModÃ¨le ALS de base
    als = ALS(
        userCol="userId",
        itemCol="movieId",
        ratingCol="rating",
        coldStartStrategy="drop",
        nonnegative=True
    )

    # 6. Grille d'hyperparamÃ¨tres
    param_grid = ParamGridBuilder() \
        .addGrid(als.rank, [8, 10, 12]) \
        .addGrid(als.regParam, [0.05, 0.1, 0.2]) \
        .addGrid(als.maxIter, [10, 15]) \
        .build()

    # 7. Ã‰valuateur RMSE
    evaluator = RegressionEvaluator(
        metricName="rmse",
        labelCol="rating",
        predictionCol="prediction"
    )

    # 8. CrossValidator
    crossval = CrossValidator(
        estimator=als,
        estimatorParamMaps=param_grid,
        evaluator=evaluator,
        numFolds=3,
        parallelism=2
    )

    # 9. EntraÃ®nement + sÃ©lection des meilleurs hyperparamÃ¨tres
    cv_model = crossval.fit(train)
    best_model = cv_model.bestModel

    print("\nâœ”ï¸ Meilleurs hyperparamÃ¨tres trouvÃ©s :")
    print("  â†’ rank     =", best_model.rank)
    print("  â†’ regParam =", best_model._java_obj.parent().getRegParam())
    print("  â†’ maxIter  =", best_model._java_obj.parent().getMaxIter())

    # 10. Ã‰valuation finale sur test
    preds = best_model.transform(test)
    rmse = evaluator.evaluate(preds)
    print(f"\nğŸ“‰ RMSE sur le test set = {rmse:.4f}")

    spark.stop()

if __name__ == "__main__":
    main()
