{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8607c89f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functions \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m count\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import count\n",
    "import numpy as np\n",
    "from pyspark.sql.window import Window\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e5dcb3-cfb4-45e9-b88d-6dbd7a354553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifiez si un SparkContext existe déjà\n",
    "if 'sc' in locals() or 'sc' in globals():\n",
    "    sc.stop()  # Arrêtez le SparkContext précédent\n",
    "    \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ExploreRatingCSV\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ed0ed5-bfdd-4929-83c4-6471010404ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "ratings_df = spark.read.csv(\"hdfs:///input/rating.csv\", header=True, inferSchema=True)\n",
    "movies_df = spark.read.csv(\"hdfs:///input/movie.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b119b73e",
   "metadata": {},
   "source": [
    "# Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9cfb07-f857-4e03-9de9-9b7d977e2839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les 5 premières lignes de ratings_df\n",
    "print(\"Premières lignes de ratings_df:\")\n",
    "ratings_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a133da-011c-4c02-9242-c701bfa586d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification des valeurs nulles dans ratings_df\n",
    "print(\"Vérification des valeurs nulles dans ratings_df :\")\n",
    "ratings_null_counts = ratings_df.select([((F.col(c).isNull()).cast(\"int\")).alias(c) for c in ratings_df.columns]) \\\n",
    "                                .agg(*[F.sum(F.col(c)).alias(c) for c in ratings_df.columns]) \\\n",
    "                                .collect()[0].asDict()\n",
    "for col_name, null_count in ratings_null_counts.items():\n",
    "    print(f\"{col_name}: {null_count} null(s)\")\n",
    "\n",
    "# Afficher les lignes contenant des valeurs nulles dans ratings_df\n",
    "print(\"\\nLignes contenant des valeurs nulles dans ratings_df :\")\n",
    "condition = None\n",
    "for c in ratings_df.columns:\n",
    "    if condition is None:\n",
    "        condition = F.col(c).isNull()\n",
    "    else:\n",
    "        condition = condition | F.col(c).isNull()\n",
    "\n",
    "ratings_df.filter(condition).show(5)\n",
    "\n",
    "# Vérification des doublons dans ratings_df\n",
    "print(\"\\nVérification des doublons dans ratings_df :\")\n",
    "duplicates_count = ratings_df.count() - ratings_df.distinct().count()\n",
    "print(f\"Nombre de doublons : {duplicates_count}\")\n",
    "ratings_df.groupBy(ratings_df.columns).count().filter(\"count > 1\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4641bf90-bae4-42ed-9c49-8ead8e09b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter le nombre d'évaluations par utilisateur\n",
    "user_rating_counts = ratings_df.groupBy(\"userId\").count()\n",
    "\n",
    "# Filtrer pour obtenir les utilisateurs ayant moins de deux évaluations\n",
    "users_with_less_than_two = user_rating_counts.filter(F.col(\"count\") < 2)\n",
    "\n",
    "# Compter le nombre total d'utilisateurs concernés\n",
    "num_users_with_less_than_two = users_with_less_than_two.count()\n",
    "\n",
    "print(f\"Nombre d'utilisateurs ayant moins de deux évaluations: {num_users_with_less_than_two}\")\n",
    "\n",
    "# Afficher quelques exemples de ces utilisateurs et leur nombre d'évaluations\n",
    "print(\"\\nExemples d'utilisateurs avec moins de deux évaluations:\")\n",
    "users_with_less_than_two.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e03a6d5-9e5e-4f7a-b5e1-f33d63366a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter le nombre de films distincts évalués par chaque utilisateur\n",
    "distinct_movie_counts = ratings_df.groupBy(\"userId\").agg(\n",
    "    F.countDistinct(\"movieId\").alias(\"distinct_movies_rated\")\n",
    ")\n",
    "\n",
    "# Trier par nombre de films distincts en ordre croissant\n",
    "sorted_users = distinct_movie_counts.orderBy(\"distinct_movies_rated\")\n",
    "\n",
    "# Trouver l'utilisateur avec le moins de films distincts évalués\n",
    "user_with_least_movies = sorted_users.first()\n",
    "\n",
    "print(f\"L'utilisateur ayant évalué le moins de films distincts est:\")\n",
    "print(f\"userId: {user_with_least_movies['userId']}, avec {user_with_least_movies['distinct_movies_rated']} film(s) évalué(s)\")\n",
    "\n",
    "# Afficher les 5 utilisateurs ayant évalué le moins de films distincts\n",
    "print(\"\\nLes 5 utilisateurs ayant évalué le moins de films distincts:\")\n",
    "sorted_users.show(5)\n",
    "\n",
    "# Pour examiner les évaluations de cet utilisateur \n",
    "user_id = user_with_least_movies['userId']\n",
    "print(f\"\\nDétail des évaluations de l'utilisateur {user_id}:\")\n",
    "ratings_df.filter(F.col(\"userId\") == user_id).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e5954d-cc51-4e35-abe2-9eacc42cdc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agréger d'abord côté Spark (faible volume, typiquement < 20 lignes)\n",
    "agg_df = ratings_df.groupBy(\"rating\").count().orderBy(\"rating\")\n",
    "\n",
    "# Convertir en pandas (ici c’est sûr que ça tient en mémoire)\n",
    "agg_pd = agg_df.toPandas()\n",
    "\n",
    "# Calculer la distribution cumulative\n",
    "agg_pd[\"percentage\"] = agg_pd[\"count\"] / agg_pd[\"count\"].sum()\n",
    "agg_pd[\"cumulative\"] = agg_pd[\"percentage\"].cumsum()\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# 📊 Distribution simple\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x=\"rating\", y=\"count\", hue=\"rating\", data=agg_pd, palette=\"viridis\", legend=False)\n",
    "plt.title('Rating Frequency Distribution')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc3ec2c",
   "metadata": {},
   "source": [
    "# Movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56902ad-a6ec-477b-bbe7-71e9b16841cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les 5 premières lignes de movies_df\n",
    "print(\"Premières lignes de movies_df:\")\n",
    "movies_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b3b480-b1ac-4157-aafc-a2f8b1152851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification des valeurs nulles dans movies_df\n",
    "print(\"\\nVérification des valeurs nulles dans movies_df :\")\n",
    "movies_null_counts = movies_df.select([((F.col(c).isNull()).cast(\"int\")).alias(c) for c in movies_df.columns]) \\\n",
    "                              .agg(*[F.sum(F.col(c)).alias(c) for c in movies_df.columns]) \\\n",
    "                              .collect()[0].asDict()\n",
    "for col_name, null_count in movies_null_counts.items():\n",
    "    print(f\"{col_name}: {null_count} null(s)\")\n",
    "\n",
    "# Afficher les lignes contenant des valeurs nulles dans movies_df\n",
    "print(\"\\nLignes contenant des valeurs nulles dans movies_df :\")\n",
    "condition = None\n",
    "for c in movies_df.columns:\n",
    "    if condition is None:\n",
    "        condition = F.col(c).isNull()\n",
    "    else:\n",
    "        condition = condition | F.col(c).isNull()\n",
    "\n",
    "movies_df.filter(condition).show(5)\n",
    "\n",
    "# Vérification des doublons dans movies_df\n",
    "print(\"\\nVérification des doublons dans movies_df :\")\n",
    "duplicates_count = movies_df.count() - movies_df.distinct().count()\n",
    "print(f\"Nombre de doublons : {duplicates_count}\")\n",
    "movies_df.groupBy(movies_df.columns).count().filter(\"count > 1\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086049ff-d86b-4263-b1ed-f62a1541da54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activité des utilisateurs : nombre de notes par user\n",
    "user_activity_df = ratings_df.groupBy(\"userId\").agg(count(\"*\").alias(\"rating_count\"))\n",
    "user_activity_pd = user_activity_df.toPandas()\n",
    "\n",
    "# Popularité des films : nombre de notes par film\n",
    "movie_popularity_df = ratings_df.groupBy(\"movieId\").agg(count(\"*\").alias(\"rating_count\"))\n",
    "movie_popularity_pd = movie_popularity_df.toPandas()\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# 📊 Activité utilisateurs - avec labels améliorés et nouvelle couleur\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(np.log10(user_activity_pd[\"rating_count\"]), bins=50, kde=True, color='steelblue')\n",
    "plt.title('Distribution de l\\'activité des utilisateurs', fontsize=13, fontweight='bold')\n",
    "plt.xlabel('Log10(Nombre d\\'évaluations par utilisateur)')\n",
    "plt.ylabel('Nombre d\\'utilisateurs')\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "plt.annotate('Échelle logarithmique', xy=(0.05, 0.95), xycoords='axes fraction', \n",
    "             fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.8))\n",
    "\n",
    "# 📊 Popularité des films - avec labels améliorés et nouvelle couleur\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(np.log10(movie_popularity_pd[\"rating_count\"]), bins=50, kde=True, color='darkorange')\n",
    "plt.title('Distribution de la popularité des films', fontsize=13, fontweight='bold')\n",
    "plt.xlabel('Log10(Nombre d\\'évaluations par film)')\n",
    "plt.ylabel('Nombre de films')\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "plt.annotate('Échelle logarithmique', xy=(0.05, 0.95), xycoords='axes fraction', \n",
    "             fontsize=10, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Analyse des distributions dans le dataset MovieLens', fontsize=15, y=1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35c3ba6",
   "metadata": {},
   "source": [
    "Analyse des Distributions du Dataset MovieLens\n",
    "1. Comportement des Utilisateurs\n",
    "Le graphique de gauche révèle comment les utilisateurs interagissent avec la plateforme:\n",
    "\n",
    "Distribution logarithmique: La visualisation utilise une échelle logarithmique (base 10) pour représenter efficacement la large gamme d'activités utilisateur\n",
    "Zone d'activité typique: La majorité des utilisateurs évaluent entre 30 et 300 films (10^1.5 à 10^2.5)\n",
    "Comportement médian: Le pic à 10^1.5 indique qu'un utilisateur typique évalue environ 30-40 films\n",
    "\"Super-utilisateurs\": La queue de distribution montre une minorité d'utilisateurs très actifs évaluant jusqu'à 3000+ films (10^3.5)\n",
    "\n",
    "2. Popularité des Films\n",
    "Le graphique de droite montre comment l'attention se distribue parmi les films:\n",
    "\n",
    "Forte inégalité: La courbe révèle une distribution extrêmement asymétrique avec un pic prononcé près de 10^0\n",
    "\"Longue traîne\": La grande majorité des films reçoit très peu d'évaluations (entre 1 et 10)\n",
    "Films blockbusters: Un petit nombre de films accumule des milliers d'évaluations (jusqu'à 10^4)\n",
    "Loi de puissance: Cette distribution suit typiquement une loi de puissance, caractéristique des phénomènes de popularité\n",
    "\n",
    "3. Implications pour les Systèmes de Recommandation\n",
    "Ces distributions ont des conséquences importantes pour la conception d'algorithmes efficaces:\n",
    "Défis Techniques\n",
    "\n",
    "Matrice creuse: La matrice utilisateurs-films contient principalement des valeurs manquantes, compliquant l'analyse\n",
    "Signal-bruit: Les films peu évalués offrent un signal statistique faible pour les prédictions\n",
    "Déséquilibre d'information: Les décisions sont souvent biaisées vers les items populaires et les utilisateurs actifs\n",
    "\n",
    "Stratégies de Correction\n",
    "\n",
    "Normalisation des données: Pondérer les évaluations pour réduire l'influence disproportionnée des super-utilisateurs\n",
    "Régularisation: Appliquer des techniques comme la régularisation L2 dans les modèles de factorisation matricielle\n",
    "Diversification forcée: Intégrer des mécanismes pour promouvoir les contenus de la longue traîne\n",
    "Approches hybrides: Combiner le filtrage collaboratif avec des méthodes basées sur le contenu moins sensibles aux biais\n",
    "\n",
    "Opportunités\n",
    "\n",
    "Segmentation d'utilisateurs: Identifier des groupes d'utilisateurs avec différents niveaux d'engagement\n",
    "Découvrabilité: Créer des mécanismes pour aider les utilisateurs à découvrir des films de niche\n",
    "Personnalisation avancée: Exploiter les données des super-utilisateurs comme source d'information riche pour améliorer les recommandations pour tous\n",
    "\n",
    "Ces distributions asymétriques sont caractéristiques des systèmes de recommandation et représentent à la fois un défi et une opportunité pour développer des algorithmes plus sophistiqués et équitables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bb619e-5b13-484c-832a-39494f817f30",
   "metadata": {},
   "source": [
    "# Ponderation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd13a7c0-66e1-432b-acca-30ecd730543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n",
    "\n",
    "# ====================== INITIALISATION ======================\n",
    "# Vérifiez si un SparkContext existe déjà\n",
    "if 'sc' in locals() or 'sc' in globals():\n",
    "    sc.stop()  # Arrêtez le SparkContext précédent\n",
    "    \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WeightedRatingWithBias\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Chemin de sortie pour les données pondérées\n",
    "output_path = \"hdfs:///processed/weighted_ratings.csv\"\n",
    "\n",
    "# ====================== CHARGEMENT DES DONNÉES ======================\n",
    "print(\"📥 Chargement des données...\")\n",
    "\n",
    "# Charger les données de ratings et films (assurez-vous que les fichiers existent)\n",
    "ratings_df = spark.read.csv(\"hdfs:///input/rating.csv\", header=True, inferSchema=True)\n",
    "movies_df = spark.read.csv(\"hdfs:///input/movie.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Vérification du chargement\n",
    "print(f\"✅ Données de ratings chargées: {ratings_df.count()} lignes\")\n",
    "print(f\"✅ Données de films chargées: {movies_df.count()} lignes\")\n",
    "\n",
    "# ====================== CALCUL DES STATISTIQUES UTILISATEUR ET FILM ======================\n",
    "print(\"🧮 Calcul des statistiques pour la pondération...\")\n",
    "\n",
    "# Calcul des statistiques par utilisateur\n",
    "user_stats = ratings_df.groupBy(\"userId\").agg(\n",
    "    F.count(\"rating\").alias(\"user_count\"),\n",
    "    F.avg(\"rating\").alias(\"user_mean\"),\n",
    "    F.stddev(\"rating\").alias(\"user_stddev\")\n",
    ")\n",
    "\n",
    "# Calcul des statistiques par film\n",
    "movie_stats = ratings_df.groupBy(\"movieId\").agg(\n",
    "    F.count(\"rating\").alias(\"movie_count\"),\n",
    "    F.avg(\"rating\").alias(\"movie_avg_rating\")\n",
    ")\n",
    "\n",
    "# Vérification des statistiques\n",
    "print(f\"✅ Statistiques utilisateur calculées : {user_stats.count()} utilisateurs\")\n",
    "print(f\"✅ Statistiques film calculées : {movie_stats.count()} films\")\n",
    "\n",
    "# ====================== APPLICATION DE LA PONDERATION ======================\n",
    "print(\"⚖️ Application des pondérations et ajustements...\")\n",
    "\n",
    "# Joindre les statistiques utilisateur et film avec les données de ratings\n",
    "weighted_df = ratings_df.join(user_stats, on=\"userId\") \\\n",
    "                        .join(movie_stats, on=\"movieId\") \\\n",
    "                        .join(movies_df, on=\"movieId\")  # Joindre les informations du film\n",
    "\n",
    "print(f\"✅ Données jointes avec les statistiques: {weighted_df.count()} lignes\")\n",
    "\n",
    "# Calcul des évaluations normalisées et des poids\n",
    "processed_df = weighted_df.withColumn(\n",
    "    # Normalisation Z-score par utilisateur\n",
    "    \"normalized_rating\", \n",
    "    (F.col(\"rating\") - F.col(\"user_mean\")) / \n",
    "    F.when(F.col(\"user_stddev\") > 0, F.col(\"user_stddev\")).otherwise(1.0)\n",
    ").withColumn(\n",
    "    # Poids utilisateur (inversement proportionnel à l'activité)\n",
    "    \"user_weight\", \n",
    "    1.0 / F.log1p(F.col(\"user_count\"))\n",
    ").withColumn(\n",
    "    # Poids film (inversement proportionnel à la popularité)\n",
    "    \"movie_weight\", \n",
    "    1.0 / F.log1p(F.col(\"movie_count\"))\n",
    ").withColumn(\n",
    "    # Confiance combinée\n",
    "    \"confidence\", \n",
    "    F.col(\"user_weight\") * F.col(\"movie_weight\")\n",
    ")\n",
    "\n",
    "print(\"✅ Pondération appliquée avec succès\")\n",
    "\n",
    "# ====================== VÉRIFICATION DE LA PONDERATION ======================\n",
    "print(\"\\n📈 Statistiques des pondérations :\")\n",
    "processed_df.select(\n",
    "    F.min(\"confidence\").alias(\"min_confidence\"),\n",
    "    F.max(\"confidence\").alias(\"max_confidence\"),\n",
    "    F.avg(\"confidence\").alias(\"avg_confidence\"),\n",
    "    F.min(\"normalized_rating\").alias(\"min_norm_rating\"),\n",
    "    F.max(\"normalized_rating\").alias(\"max_norm_rating\")\n",
    ").show()\n",
    "\n",
    "# ====================== SÉLECTION ET ENREGISTREMENT DES DONNÉES FINAL ======================\n",
    "# Sélectionner les colonnes importantes pour l'enregistrement final\n",
    "final_df = processed_df.select(\n",
    "    \"userId\", \n",
    "    \"movieId\",\n",
    "    \"rating\",\n",
    "    \"normalized_rating\",\n",
    "    \"confidence\",\n",
    "    \"user_mean\",\n",
    "    \"user_stddev\",\n",
    "    \"movie_count\",\n",
    "    \"title\",      # du fichier movies.csv\n",
    "    \"genres\",     # du fichier movies.csv\n",
    "    \"timestamp\"   # garder le timestamp pour pouvoir faire des splits temporels\n",
    ")\n",
    "\n",
    "# Enregistrement des données pondérées dans le répertoire HDFS\n",
    "print(f\"💾 Enregistrement du fichier CSV pondéré dans {output_path}...\")\n",
    "\n",
    "# Vérifier si le répertoire existe et le supprimer s'il existe déjà\n",
    "try:\n",
    "    spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration()).delete(\n",
    "        spark._jvm.org.apache.hadoop.fs.Path(output_path), True)\n",
    "except:\n",
    "    pass  # Le chemin n'existe pas, on continue\n",
    "\n",
    "# Enregistrer en CSV\n",
    "final_df.write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "print(\"✅ Prétraitement terminé! Fichier pondéré prêt à être utilisé.\")\n",
    "\n",
    "# ====================== AFFICHAGE D'UN APERÇU DES DONNÉES PONDERÉES ======================\n",
    "print(\"\\n📊 Aperçu des données pondérées:\")\n",
    "final_df.select(\"userId\", \"movieId\", \"title\", \"rating\", \"normalized_rating\", \"confidence\").show(5)\n",
    "\n",
    "# ====================== STATISTIQUES SUPPLÉMENTAIRES ======================\n",
    "print(\"\\n📈 Statistiques supplémentaires sur les données pondérées :\")\n",
    "final_df.select(\n",
    "    F.avg(\"confidence\").alias(\"avg_confidence\"),\n",
    "    F.countDistinct(\"userId\").alias(\"unique_users\"),\n",
    "    F.countDistinct(\"movieId\").alias(\"unique_movies\"),\n",
    "    F.avg(\"normalized_rating\").alias(\"avg_normalized_rating\")\n",
    ").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
