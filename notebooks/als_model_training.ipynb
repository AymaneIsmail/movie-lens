{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64d4b225",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02c32090",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ====================== INITIALISATION ======================\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TrainALSModel\") \\\n",
    "    .master(\"spark://namenode:7077\") \\\n",
    "    .config(\"spark.cassandra.connection.host\", \"cassandra\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acbe9a61",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Lecture des fichiers CSV depuis HDFS...\n"
     ]
    }
   ],
   "source": [
    "# ====================== CHARGEMENT DES DONNÃ‰ES ======================\n",
    "ratings_schema = StructType([\n",
    "    StructField(\"userId\", IntegerType(), True),\n",
    "    StructField(\"movieId\", IntegerType(), True),\n",
    "    StructField(\"rating\", FloatType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "])\n",
    "\n",
    "movies_schema = StructType([\n",
    "    StructField(\"movieId\", IntegerType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"genres\", StringType(), True),\n",
    "])\n",
    "\n",
    "print(\"ðŸ“¥ Lecture des fichiers CSV depuis HDFS...\")\n",
    "ratings_df = spark.read.csv(\"hdfs:///input/rating.csv\", header=True, schema=ratings_schema)\n",
    "movies_df = spark.read.csv(\"hdfs:///input/movie.csv\", header=True, schema=movies_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62bbce19",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ====================== NETTOYAGE ======================\n",
    "# Supprimer les lignes avec valeurs nulles\n",
    "ratings_df = ratings_df.dropna(subset=[\"userId\", \"movieId\", \"rating\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1940497a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ====================== DIVISION TRAIN / TEST ======================\n",
    "train_df, test_df = ratings_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "242e2e00",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– EntraÃ®nement du modÃ¨le ALS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/30 11:42:09 WARN HeartbeatReceiver: Removing executor 2 with no recent heartbeats: 152963 ms exceeds timeout 120000 ms\n",
      "25/04/30 11:42:09 WARN HeartbeatReceiver: Removing executor 5 with no recent heartbeats: 167371 ms exceeds timeout 120000 ms\n",
      "25/04/30 11:42:09 WARN HeartbeatReceiver: Removing executor 12 with no recent heartbeats: 156997 ms exceeds timeout 120000 ms\n",
      "25/04/30 11:42:09 WARN HeartbeatReceiver: Removing executor 8 with no recent heartbeats: 167422 ms exceeds timeout 120000 ms\n",
      "25/04/30 11:42:09 WARN HeartbeatReceiver: Removing executor 7 with no recent heartbeats: 167494 ms exceeds timeout 120000 ms\n",
      "25/04/30 11:42:09 WARN HeartbeatReceiver: Removing executor 4 with no recent heartbeats: 167857 ms exceeds timeout 120000 ms\n",
      "25/04/30 11:42:09 WARN HeartbeatReceiver: Removing executor 11 with no recent heartbeats: 156053 ms exceeds timeout 120000 ms\n",
      "25/04/30 11:42:09 WARN HeartbeatReceiver: Removing executor 6 with no recent heartbeats: 154289 ms exceeds timeout 120000 ms\n",
      "25/04/30 11:42:09 WARN HeartbeatReceiver: Removing executor 9 with no recent heartbeats: 167093 ms exceeds timeout 120000 ms\n",
      "25/04/30 11:42:09 WARN HeartbeatReceiver: Removing executor 0 with no recent heartbeats: 156334 ms exceeds timeout 120000 ms\n",
      "25/04/30 11:42:09 WARN HeartbeatReceiver: Removing executor 10 with no recent heartbeats: 155722 ms exceeds timeout 120000 ms\n",
      "25/04/30 11:42:09 WARN HeartbeatReceiver: Removing executor 3 with no recent heartbeats: 154228 ms exceeds timeout 120000 ms\n",
      "25/04/30 11:42:09 WARN HeartbeatReceiver: Removing executor 13 with no recent heartbeats: 167139 ms exceeds timeout 120000 ms\n",
      "25/04/30 11:42:09 ERROR TransportRequestHandler: Error sending result RpcResponse[requestId=6704154687197498501,body=NioManagedBuffer[buf=java.nio.HeapByteBuffer[pos=0 lim=81 cap=156]]] to /172.18.0.6:37940; closing connection\n",
      "java.io.IOException: Broken pipe\n",
      "\tat java.base/sun.nio.ch.FileDispatcherImpl.write0(Native Method)\n",
      "\tat java.base/sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:62)\n",
      "\tat java.base/sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:132)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:97)\n",
      "\tat java.base/sun.nio.ch.IOUtil.write(IOUtil.java:53)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:532)\n",
      "\tat org.apache.spark.network.protocol.MessageWithHeader.copyByteBuf(MessageWithHeader.java:148)\n",
      "\tat org.apache.spark.network.protocol.MessageWithHeader.transferTo(MessageWithHeader.java:123)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doWriteFileRegion(NioSocketChannel.java:369)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel.doWriteInternal(AbstractNioByteChannel.java:238)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel.doWrite0(AbstractNioByteChannel.java:212)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doWrite(NioSocketChannel.java:407)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.flush0(AbstractChannel.java:931)\n",
      "\tat io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.flush0(AbstractNioChannel.java:354)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.flush(AbstractChannel.java:895)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.flush(DefaultChannelPipeline.java:1372)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:921)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeFlush(AbstractChannelHandlerContext.java:907)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.flush(AbstractChannelHandlerContext.java:893)\n",
      "\tat io.netty.channel.ChannelDuplexHandler.flush(ChannelDuplexHandler.java:127)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeFlush0(AbstractChannelHandlerContext.java:923)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:941)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1247)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:566)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/04/30 11:42:09 ERROR Inbox: Ignoring error\n",
      "java.lang.AssertionError: assertion failed: BlockManager re-registration shouldn't succeed when the executor is lost\n",
      "\tat scala.Predef$.assert(Predef.scala:223)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:727)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/04/30 11:42:09 ERROR Inbox: Ignoring error\n",
      "java.lang.AssertionError: assertion failed: BlockManager re-registration shouldn't succeed when the executor is lost\n",
      "\tat scala.Predef$.assert(Predef.scala:223)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:727)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/04/30 11:42:10 ERROR Inbox: Ignoring error\n",
      "java.lang.AssertionError: assertion failed: BlockManager re-registration shouldn't succeed when the executor is lost\n",
      "\tat scala.Predef$.assert(Predef.scala:223)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:727)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/04/30 11:42:10 ERROR TaskSchedulerImpl: Lost executor 2 on 172.18.0.6: Executor heartbeat timed out after 152963 ms\n",
      "25/04/30 11:42:10 WARN TaskSetManager: Lost task 3.0 in stage 1.0 (TID 4) (172.18.0.6 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 152963 ms\n",
      "25/04/30 11:42:10 WARN TaskSetManager: Lost task 17.0 in stage 1.0 (TID 18) (172.18.0.6 executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 152963 ms\n",
      "25/04/30 11:42:10 ERROR TaskSchedulerImpl: Lost executor 5 on 172.18.0.6: Executor heartbeat timed out after 167371 ms\n",
      "25/04/30 11:42:10 WARN TaskSetManager: Lost task 22.0 in stage 1.0 (TID 23) (172.18.0.6 executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 167371 ms\n",
      "25/04/30 11:42:10 WARN TaskSetManager: Lost task 8.0 in stage 1.0 (TID 9) (172.18.0.6 executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 167371 ms\n",
      "25/04/30 11:42:10 ERROR TaskSchedulerImpl: Lost executor 12 on 172.18.0.8: Executor heartbeat timed out after 156997 ms\n",
      "25/04/30 11:42:10 WARN TaskSetManager: Lost task 13.0 in stage 1.0 (TID 14) (172.18.0.8 executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 156997 ms\n",
      "25/04/30 11:42:10 WARN TaskSetManager: Lost task 27.0 in stage 1.0 (TID 28) (172.18.0.8 executor 12): ExecutorLostFailure (executor 12 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 156997 ms\n",
      "25/04/30 11:42:10 ERROR TaskSchedulerImpl: Lost executor 8 on 172.18.0.8: Executor heartbeat timed out after 167422 ms\n",
      "25/04/30 11:42:10 WARN TaskSetManager: Lost task 4.0 in stage 1.0 (TID 5) (172.18.0.8 executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 167422 ms\n",
      "25/04/30 11:42:10 WARN TaskSetManager: Lost task 18.0 in stage 1.0 (TID 19) (172.18.0.8 executor 8): ExecutorLostFailure (executor 8 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 167422 ms\n",
      "25/04/30 11:42:10 ERROR TaskSchedulerImpl: Lost executor 7 on 172.18.0.8: Executor heartbeat timed out after 167494 ms\n",
      "25/04/30 11:42:10 WARN TaskSetManager: Lost task 1.0 in stage 1.0 (TID 2) (172.18.0.8 executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 167494 ms\n",
      "25/04/30 11:42:10 WARN TaskSetManager: Lost task 15.0 in stage 1.0 (TID 16) (172.18.0.8 executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 167494 ms\n",
      "25/04/30 11:42:10 ERROR Inbox: Ignoring error\n",
      "java.lang.AssertionError: assertion failed: BlockManager re-registration shouldn't succeed when the executor is lost\n",
      "\tat scala.Predef$.assert(Predef.scala:223)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:727)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/04/30 11:42:10 ERROR Inbox: Ignoring error\n",
      "java.lang.AssertionError: assertion failed: BlockManager re-registration shouldn't succeed when the executor is lost\n",
      "\tat scala.Predef$.assert(Predef.scala:223)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:727)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/04/30 11:42:10 ERROR TaskSchedulerImpl: Lost executor 4 on 172.18.0.6: Executor heartbeat timed out after 167857 ms\n",
      "25/04/30 11:42:10 WARN TaskSetManager: Lost task 12.0 in stage 1.0 (TID 13) (172.18.0.6 executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 167857 ms\n",
      "25/04/30 11:42:10 WARN TaskSetManager: Lost task 26.0 in stage 1.0 (TID 27) (172.18.0.6 executor 4): ExecutorLostFailure (executor 4 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 167857 ms\n",
      "25/04/30 11:42:10 ERROR TaskSchedulerImpl: Lost executor 11 on 172.18.0.8: Executor heartbeat timed out after 156053 ms\n",
      "25/04/30 11:42:10 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1) (172.18.0.8 executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 156053 ms\n",
      "25/04/30 11:42:10 WARN TaskSetManager: Lost task 14.0 in stage 1.0 (TID 15) (172.18.0.8 executor 11): ExecutorLostFailure (executor 11 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 156053 ms\n",
      "25/04/30 11:42:10 ERROR TaskSchedulerImpl: Lost executor 6 on 172.18.0.6: Executor heartbeat timed out after 154289 ms\n",
      "25/04/30 11:42:10 WARN TaskSetManager: Lost task 6.0 in stage 1.0 (TID 7) (172.18.0.6 executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 154289 ms\n",
      "25/04/30 11:42:10 WARN TaskSetManager: Lost task 20.0 in stage 1.0 (TID 21) (172.18.0.6 executor 6): ExecutorLostFailure (executor 6 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 154289 ms\n",
      "25/04/30 11:42:10 ERROR TaskSchedulerImpl: Lost executor 9 on 172.18.0.8: Executor heartbeat timed out after 167093 ms\n",
      "25/04/30 11:42:10 WARN TaskSetManager: Lost task 16.0 in stage 1.0 (TID 17) (172.18.0.8 executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 167093 ms\n",
      "25/04/30 11:42:10 WARN TaskSetManager: Lost task 2.0 in stage 1.0 (TID 3) (172.18.0.8 executor 9): ExecutorLostFailure (executor 9 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 167093 ms\n",
      "25/04/30 11:42:10 ERROR Inbox: Ignoring error\n",
      "java.lang.AssertionError: assertion failed: BlockManager re-registration shouldn't succeed when the executor is lost\n",
      "\tat scala.Predef$.assert(Predef.scala:223)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:727)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/04/30 11:42:10 ERROR TaskSchedulerImpl: Lost executor 0 on 172.18.0.6: Executor heartbeat timed out after 156334 ms\n",
      "25/04/30 11:42:10 WARN TaskSetManager: Lost task 10.0 in stage 1.0 (TID 11) (172.18.0.6 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 156334 ms\n",
      "25/04/30 11:42:10 WARN TaskSetManager: Lost task 24.0 in stage 1.0 (TID 25) (172.18.0.6 executor 0): ExecutorLostFailure (executor 0 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 156334 ms\n",
      "25/04/30 11:42:10 ERROR Inbox: Ignoring error\n",
      "java.lang.AssertionError: assertion failed: BlockManager re-registration shouldn't succeed when the executor is lost\n",
      "\tat scala.Predef$.assert(Predef.scala:223)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:727)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/04/30 11:42:10 ERROR TaskSchedulerImpl: Lost executor 10 on 172.18.0.8: Executor heartbeat timed out after 155722 ms\n",
      "25/04/30 11:42:10 WARN TaskSetManager: Lost task 9.0 in stage 1.0 (TID 10) (172.18.0.8 executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 155722 ms\n",
      "25/04/30 11:42:10 WARN TaskSetManager: Lost task 23.0 in stage 1.0 (TID 24) (172.18.0.8 executor 10): ExecutorLostFailure (executor 10 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 155722 ms\n",
      "25/04/30 11:42:10 ERROR TaskSchedulerImpl: Lost executor 3 on 172.18.0.6: Executor heartbeat timed out after 154228 ms\n",
      "25/04/30 11:42:10 ERROR Inbox: Ignoring error\n",
      "java.lang.AssertionError: assertion failed: BlockManager re-registration shouldn't succeed when the executor is lost\n",
      "\tat scala.Predef$.assert(Predef.scala:223)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:727)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/04/30 11:42:10 WARN TaskSetManager: Lost task 7.0 in stage 1.0 (TID 8) (172.18.0.6 executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 154228 ms\n",
      "25/04/30 11:42:10 WARN TaskSetManager: Lost task 21.0 in stage 1.0 (TID 22) (172.18.0.6 executor 3): ExecutorLostFailure (executor 3 exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 154228 ms\n",
      "25/04/30 11:42:10 ERROR TaskSchedulerImpl: Lost executor 13 on 172.18.0.8: Executor heartbeat timed out after 167139 ms\n",
      "25/04/30 11:42:11 ERROR TaskSchedulerImpl: Lost executor 1 on 172.18.0.6: Command exited with code 0\n",
      "25/04/30 11:42:11 WARN TaskSetManager: Lost task 19.0 in stage 1.0 (TID 20) (172.18.0.6 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Command exited with code 0\n",
      "25/04/30 11:42:11 WARN TaskSetManager: Lost task 5.0 in stage 1.0 (TID 6) (172.18.0.6 executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Command exited with code 0\n",
      "25/04/30 11:42:11 ERROR TaskSchedulerImpl: Ignoring update with state RUNNING for TID 29 because its task set is gone (this is likely the result of receiving duplicate task finished status updates) or its executor has been marked as failed.\n",
      "25/04/30 11:42:11 ERROR TaskSchedulerImpl: Ignoring update with state RUNNING for TID 30 because its task set is gone (this is likely the result of receiving duplicate task finished status updates) or its executor has been marked as failed.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ====================== ENTRAÃŽNEMENT ======================\n",
    "print(\"ðŸ¤– EntraÃ®nement du modÃ¨le ALS...\")\n",
    "als = ALS(\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"movieId\",\n",
    "    ratingCol=\"rating\",\n",
    "    nonnegative=True,\n",
    "    implicitPrefs=False,\n",
    "    coldStartStrategy=\"drop\",  # pour Ã©viter les NaN en test\n",
    "    rank=12,\n",
    "    maxIter=15,\n",
    "    regParam=0.05\n",
    ")\n",
    "\n",
    "model = als.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2e15fb1",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Ã‰valuation du modÃ¨le...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 184:=====================================================> (28 + 1) / 29]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RMSE sur l'ensemble test : 0.7926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ====================== Ã‰VALUATION ======================\n",
    "print(\"ðŸ“Š Ã‰valuation du modÃ¨le...\")\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"rating\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"âœ… RMSE sur l'ensemble test : {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f840c1e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Sauvegarde du modÃ¨le dans HDFS (/models/als)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 253:=======================================>                (7 + 3) / 10]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ‰ ModÃ¨le entraÃ®nÃ© et sauvegardÃ© avec succÃ¨s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ====================== ENREGISTREMENT ======================\n",
    "print(\"ðŸ’¾ Sauvegarde du modÃ¨le dans HDFS (/models/als)...\")\n",
    "model.write().overwrite().save(\"hdfs:///models/als\")\n",
    "\n",
    "print(\"ðŸŽ‰ ModÃ¨le entraÃ®nÃ© et sauvegardÃ© avec succÃ¨s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcabd3e6-67b8-40e7-9191-dac5cfc522d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
